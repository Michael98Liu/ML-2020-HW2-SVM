{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code is based on\n",
    "# https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/supervised_learning/adaboost.py\n",
    "# with modification\n",
    "\n",
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Decision stump used as weak classifier\n",
    "class DecisionStump():\n",
    "    def __init__(self):\n",
    "        # Determines if sample shall be classified as -1 or 1 given threshold\n",
    "        self.polarity = 1\n",
    "        # The index of the feature used to make classification\n",
    "        self.feature_index = None\n",
    "        # The threshold value that the feature should be measured against\n",
    "        self.threshold = None\n",
    "        # Value indicative of the classifier's accuracy\n",
    "        self.alpha = None\n",
    "\n",
    "class Adaboost():\n",
    "    \"\"\"Boosting method that uses a number of weak classifiers in \n",
    "    ensemble to make a strong classifier. This implementation uses decision\n",
    "    stumps, which is a one level Decision Tree. \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_clf: int\n",
    "        The number of weak classifiers that will be used. \n",
    "    \"\"\"\n",
    "    def __init__(self, n_clf=5):\n",
    "        self.n_clf = n_clf\n",
    "\n",
    "    def fit(self, X, y, loss='exponential'):\n",
    "        assert(loss in ['exponential', 'logistic'])\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        # Initialize weights to 1/N\n",
    "        w = np.full(n_samples, (1 / n_samples))\n",
    "        \n",
    "        self.clfs = []\n",
    "        # Iterate through classifiers\n",
    "        for _ in range(self.n_clf):\n",
    "            clf = DecisionStump()\n",
    "            # Minimum error given for using a certain feature value threshold\n",
    "            # for predicting sample label\n",
    "            min_error = float('inf')\n",
    "            # Iterate throught every unique feature value and see what value\n",
    "            # makes the best threshold for predicting y\n",
    "            for feature_i in range(n_features):\n",
    "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "                # Try every unique feature value as threshold\n",
    "                for threshold in unique_values:\n",
    "                    p = 1\n",
    "                    # Set all predictions to '1' initially\n",
    "                    prediction = np.ones(np.shape(y))\n",
    "                    # Label the samples whose values are below threshold as '-1'\n",
    "                    prediction[X[:, feature_i] < threshold] = -1\n",
    "                    # Error = sum of weights of misclassified samples\n",
    "                    error = sum(w[y != prediction])\n",
    "                    \n",
    "                    # If the error is over 50% we flip the polarity\n",
    "                    if error > 0.5:\n",
    "                        error = 1 - error\n",
    "                        p = -1\n",
    "\n",
    "                    # If this threshold resulted in the smallest error we save the\n",
    "                    # configuration\n",
    "                    if error < min_error:\n",
    "                        clf.polarity = p\n",
    "                        clf.threshold = threshold\n",
    "                        clf.feature_index = feature_i\n",
    "                        min_error = error\n",
    "            \n",
    "            clf.alpha = 0.5 * math.log((1.0 - min_error) / (min_error + 1e-10))     \n",
    "            self.clfs.append(clf)\n",
    "            # print(clf.feature_index, clf.alpha)\n",
    "            \n",
    "            if loss == 'exponential':\n",
    "                predictions = np.ones(np.shape(y)) # initialize\n",
    "                negative_idx = (clf.polarity * X[:, clf.feature_index] < clf.polarity * clf.threshold)\n",
    "                predictions[negative_idx] = -1\n",
    "                w *= np.exp(-clf.alpha * y * predictions)\n",
    "                # the same as: w = np.exp(-y * predictions)\n",
    "            elif loss == 'logistic':\n",
    "                sign, predictions = self.predict(X)\n",
    "                w = 1/(1+np.exp(y * predictions))\n",
    "            # Normalize to one\n",
    "            w /= np.sum(w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = np.shape(X)[0]\n",
    "        y_pred = np.zeros((n_samples, 1))\n",
    "        # For each classifier => label the samples\n",
    "        for clf in self.clfs:\n",
    "            # Set all predictions to '1' initially\n",
    "            predictions = np.ones(np.shape(y_pred))\n",
    "            # The indexes where the sample values are below threshold\n",
    "            negative_idx = (clf.polarity * X[:, clf.feature_index] < clf.polarity * clf.threshold)\n",
    "            # Label those as '-1'\n",
    "            predictions[negative_idx] = -1\n",
    "            # Add predictions weighted by the classifiers alpha\n",
    "            # (alpha indicative of classifier's proficiency)\n",
    "            y_pred += clf.alpha * predictions\n",
    "\n",
    "        # Return sign of prediction sum\n",
    "        y_pred_sign = np.sign(y_pred).flatten()\n",
    "\n",
    "        return y_pred_sign, y_pred.flatten()\n",
    "    \n",
    "    def score(self, y_pred, y_true):\n",
    "        assert(y_pred.shape == y_true.shape)\n",
    "        return np.sum(y_pred==y_true, axis=0)/y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"abalone.data\",names=['sex','length','diameter','height','whole weight','shucked weight',\n",
    "                                        'viscera weight','shell weight','rings'])\n",
    "data = data.assign(sex=data.sex.apply(lambda x: 1 if x=='M' else (-1 if x=='F' else 0)))\n",
    "data = data.assign(rings=data.rings.apply(lambda x: 1 if x <=9 else -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3133, 9), (1044, 9))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val = data.values[:3133]\n",
    "test = data.values[3133:]\n",
    "train_val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_x = train_val[:,:-1]\n",
    "train_val_y = train_val[:,-1]\n",
    "\n",
    "test_x = test[:,:-1]\n",
    "test_y = test[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c7b978b4044634b3b1d7f086507471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.7353837687451275 [0.7257, 0.748, 0.7751, 0.7284, 0.6997]\n",
      "20 0.7318755063668465 [0.7257, 0.7496, 0.7544, 0.73, 0.6997]\n",
      "30 0.7376263050888913 [0.7257, 0.7496, 0.7544, 0.7572, 0.7013]\n",
      "40 0.7366693672898482 [0.7209, 0.7496, 0.7544, 0.7572, 0.7013]\n",
      "50 0.7366693672898482 [0.7209, 0.7496, 0.7544, 0.7572, 0.7013]\n",
      "100 0.7376273241919786 [0.7209, 0.7512, 0.7544, 0.7604, 0.7013]\n",
      "200 0.7376273241919786 [0.7209, 0.7512, 0.7544, 0.7604, 0.7013]\n",
      "300 0.7369893656592833 [0.7209, 0.7512, 0.7512, 0.7604, 0.7013]\n",
      "400 0.7369893656592833 [0.7209, 0.7512, 0.7512, 0.7604, 0.7013]\n",
      "\n",
      "Number of classifiers: 100\n",
      "Cross validation error: 0.7376273241919786\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "best_score = -1\n",
    "best_T = 0\n",
    "\n",
    "for T in tqdm([10, 20, 30, 40, 50, 100, 200, 300, 400]):\n",
    "    scores = []\n",
    "    \n",
    "    for train_ind, val_ind in kf.split(train_val):\n",
    "        \n",
    "        train = train_val[train_ind]\n",
    "        val = train_val[val_ind]\n",
    "        \n",
    "        train_x = train[:,:-1]\n",
    "        train_y = train[:,-1]\n",
    "        val_x = val[:,:-1]\n",
    "        val_y = val[:,-1]\n",
    "        \n",
    "        clf = Adaboost(n_clf=T)\n",
    "        clf.fit(train_x, train_y, loss='logistic')\n",
    "        y_pred, y_pred_raw = clf.predict(val_x)\n",
    "        \n",
    "        score = clf.score(y_pred, val_y)\n",
    "        scores.append(score)\n",
    "        \n",
    "    avg_score = sum(scores)/5\n",
    "    print(f\"{T} {avg_score} {[round(s, 4) for s in scores]}\")\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_T = T\n",
    "\n",
    "print(f\"Number of classifiers: {best_T}\\nCross validation error: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7624521072796935\n"
     ]
    }
   ],
   "source": [
    "clf = Adaboost(n_clf=100)\n",
    "clf.fit(train_val_x, train_val_y, loss='logistic')\n",
    "y_pred, y_pred_raw = clf.predict(test_x)\n",
    "score = clf.score(y_pred, test_y)\n",
    "print(f'Test accuracy: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5125f4a395274ddcbb9c945710e1d5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.7363412160957142 [0.7257, 0.7496, 0.7767, 0.7284, 0.7013]\n",
      "20 0.7395422188931521 [0.7257, 0.7496, 0.7576, 0.7284, 0.7364]\n",
      "30 0.7395422188931521 [0.7257, 0.7496, 0.7576, 0.7284, 0.7364]\n",
      "40 0.7389037508089131 [0.7257, 0.748, 0.7576, 0.7284, 0.7348]\n",
      "50 0.7427376166236097 [0.7257, 0.748, 0.7576, 0.7476, 0.7348]\n",
      "100 0.7315840428838579 [0.7592, 0.7512, 0.6316, 0.7476, 0.7684]\n",
      "200 0.677029416410617 [0.7592, 0.7496, 0.3892, 0.7188, 0.7684]\n",
      "300 0.677029416410617 [0.7592, 0.7496, 0.3892, 0.7188, 0.7684]\n",
      "400 0.677029416410617 [0.7592, 0.7496, 0.3892, 0.7188, 0.7684]\n",
      "\n",
      "Number of classifiers: 50\n",
      "Cross validation error: 0.7427376166236097\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "best_score = -1\n",
    "best_T = 0\n",
    "\n",
    "for T in tqdm([10, 20, 30, 40, 50, 100, 200, 300, 400]):\n",
    "    scores = []\n",
    "    \n",
    "    for train_ind, val_ind in kf.split(train_val):\n",
    "        \n",
    "        train = train_val[train_ind]\n",
    "        val = train_val[val_ind]\n",
    "        \n",
    "        train_x = train[:,:-1]\n",
    "        train_y = train[:,-1]\n",
    "        val_x = val[:,:-1]\n",
    "        val_y = val[:,-1]\n",
    "        \n",
    "        clf = Adaboost(n_clf=T)\n",
    "        clf.fit(train_x, train_y, loss='exponential')\n",
    "        y_pred, y_pred_raw = clf.predict(val_x)\n",
    "        \n",
    "        score = clf.score(y_pred, val_y)\n",
    "        scores.append(score)\n",
    "        \n",
    "    avg_score = sum(scores)/5\n",
    "    print(f\"{T} {avg_score} {[round(s, 4) for s in scores]}\")\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_T = T\n",
    "\n",
    "print(f\"Number of classifiers: {best_T}\\nCross validation error: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7452107279693486\n"
     ]
    }
   ],
   "source": [
    "clf = Adaboost(n_clf=50)\n",
    "clf.fit(train_val_x, train_val_y, loss='exponential')\n",
    "y_pred, y_pred_raw = clf.predict(test_x)\n",
    "score = clf.score(y_pred, test_y)\n",
    "print(f'Test accuracy: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
